# Training data

This folder contains code for downloading and processing pretraining datasets for VinaSmol.

## Build the pretraining dataset

First, activate a virtual environment shell and log in to HuggingFace:

```bash
uv run bash
hf auth login
```
Your HuggingFace account needs to accept agreements for the following datasets:
- [CulturaX](https://huggingface.co/datasets/uonlp/CulturaX)
- [MathPile Commercial](https://huggingface.co/datasets/GAIR/MathPile_Commercial)

The next commands should be executed in the root folder of the repository.


## Download data

Download and preprocess the Vietnamese and English datasets by executing:

```bash
python -m vinasmol.training.dataset.datasets

cd vinasmol/training/dataset/data/english
mkdir -p ../annealing/english
mv pes2o openwebmath stackmathqa ../annealing/english
```

Around 10 GB of data will be downloaded from HuggingFace. The preprocessed datasets will be stored in `./data`.


## Filter and deduplicate data

### Prerequisites

We use the [exact-sentence-deduplication](https://github.com/google-research/deduplicate-text-datasets) repository from Google Research, which requires [Rust](https://www.rust-lang.org/tools/install).

Filtering, redaction and deduplication can be run with the following command:

```bash
ulimit 100000
python -m vinasmol.training.dataset.filtering.vietnamese
python -m vinasmol.training.dataset.filtering.english
python -m vinasmol.training.dataset.filtering.code
```

> [!WARNING]
>
> You must cache the required [KenLM](https://huggingface.co/edugp/kenlm/tree/main) models first: [oscar/vi.arpa.bin](https://huggingface.co/edugp/kenlm/tree/main/oscar) and [wikipedia/en.arpa.bin](https://huggingface.co/edugp/kenlm/tree/main/wikipedia). Otherwise the pipeline will download the same model in parallel, which is not what you want.

The final results are written into `./data/vi-all/deduped`, `./data/en-all/deduped` and `./data/code-all/deduped`. Additional logs and statistics about filters and removals can be found in the `./data/` directory.

## Prepare data for training

### Requirements

- A SmolLM2 tokenizer checkpoint ([details here](../../tokenization/README.md#extend-smollms-vocabulary-with-vietnamese))

The following script will:
1. Split the training Vietnamese, English and code datasets into train/val/test splits
2. Tokenize each split with the merged tokenizer
3. Optimize them with [LitData](https://github.com/Lightning-AI/litdata).

```bash
python -m vinasmol.training.dataset.preparation
```

The optimized data will be written in the [`./data/tokenized`](./data/tokenized) directory.


## Recipe

### English datasets

Adding English datasets to the continued pretraining mixture avoids catastrophic forgetting of the acquired language proficiency, capabilities and knowledge of the base model. 

For the initial pretraining phase, we use:
- Cosmopedia v2 from the [SmolLM Corpus](https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus)
- [FineWebEdu (deduplicated)](https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus)
- [Gutenberg-en](https://huggingface.co/datasets/OpenLLM-France/Lucie-Training-Dataset/viewer/Gutenberg-en) filtered and processed by OpenLLM-France
- [Wikipedia-en](https://huggingface.co/datasets/omarkamali/wikipedia-monthly)

Cosmopedia v2 and FineWebEdu are both part of SmolLM's training corpus. Due to the large size of these datasets and the small size of SmolLM2-360M, we inferred that the risk of overfitting was low enough to include them as replay datasets.

> [!NOTE]
> Cosmopedia v2, as a synthetic high-quality dataset, may be better suited for the annealing phase. Consider moving it to the annealing mixture in future training runs.

### Code datasets

Similar to English, we add code datasets in order to preserve SmolLM's performance on programming tasks. As a strategic choice due to limited resources, we choose to focus on Python in order to limit the number of programming languages in the corpus.

Specifically, we use the Python subset of [Starcoderdata](https://huggingface.co/datasets/bigcode/starcoderdata) retaining the examples with quality scores generated by HuggingFace's [`python-edu-scorer`](https://huggingface.co/datasets/jon-tow/starcoderdata-python-edu/blob/main/HuggingFaceTB/python-edu-scorer) higher than 3.

### Vietnamese datasets

- [Wikipedia](https://huggingface.co/datasets/omarkamali/wikipedia-monthly) (Vietnamese)
- [epfml/FineWeb2-HQ](https://huggingface.co/datasets/epfml/FineWeb2-HQ): General, high-quality web dataset.
- [CulturaX](https://huggingface.co/datasets/uonlp/CulturaX): General web dataset.
- [madlad-400_vi](https://huggingface.co/datasets/Symato/madlad-400_vi): General web dataset. Clean Vietnamese subset of [MADLAD-400](https://huggingface.co/datasets/allenai/MADLAD-400)
- [Binhvq News Corpus](https://huggingface.co/datasets/bigscience-data/roots_vi_binhvq_news_corpus): large news dataset.
- [phongmt184172/mtet](https://huggingface.co/phongmt184172/mtet): Parallel Vietnamese/English pairs.
- [doanhieung/vbpl](https://huggingface.co/doanhieung/vbpl): Vietnamâ€™s official law texts.

### Annealing datasets

For the annealing phase, we keep 60 % of the mixture with the proportions in the initial continued pretraining stage. The remaining 40 % consists of the following datasets:

- Wikipedia (English, Vietnamese)
- Binhvq News Corpus
- [CCVJ](../../../ccvj/src/ccvj/README.md), an in-house dataset of Vietnamese academic papers
- Gutenberg-en
- [FineMath 4+](https://huggingface.co/datasets/HuggingFaceTB/finemath)
- [StackMathQA](https://huggingface.co/datasets/math-ai/StackMathQA)

TODO: textbooks and large Vietnamese instruction datasets

The annealing mixture can be created by the following command:

```python
python -m vinasmol.training.dataset.annealing_data
```

### Training budget

We plan to continue the pretraining of SmolLM2 on around 2B training tokens, as for [EEVE-Korean](https://arxiv.org/abs/2402.14714). Even if EEVE-Korean has 10B parameters, its base model ([SOLAR 10.7B](https://arxiv.org/abs/2312.15166)) was likely pretrained on a little Korean before, so both factors might compensate.

## Data preparation

Our data preparation pipeline uses [datatrove](https://github.com/huggingface/datatrove). We take inspiration from [Sailcraft](https://github.com/sail-sg/sailcraft) for Vietnamese-specific filters.

### Formatting

The Vietnamese Wikipedia is not properly formatted and there are lots of sections to be removed. We can fork [wikiplaintext](https://github.com/OpenLLM-France/wikiplaintext) to adapt it for Vietnamese, process the Vietnamese Wikipedia separately, save it locally and use it instead. This separation can help with licensing.

In the meantime, [VietGPT's processed Wikipedia](https://huggingface.co/datasets/vietgpt/wikipedia_vi) is a good resource for prototyping as it is slightly cleaner than other sources.

### Normalization

We normalize some ad-hoc Unicode punctuation and fix text encoding using [ftfy](https://ftfy.readthedocs.io/en/latest/).

### Deduplication

We use both document deduplication and exact substring deduplication in order to remove repetitive content from the web such as copyright notices, plagiarism and so on.

For the document-level deduplication, we use MinHashLSH implemented by [Rensa](https://github.com/beowolx/rensa).

For the [exact substring deduplication](https://github.com/google-research/deduplicate-text-datasets), we use the Python implementation from `datatrove`.

### Filtering

#### URL filtering

We use URL-based filtering with the default blacklist and domain name-based heuristics of `datatrove`. We also supply a domain whitelist to avoid filtering pages on sensitive topics of informative websites such as Wikipedia.

#### Quality filtering

We use a `datatrove` pipeline with the filters used by C4, Gopher and FineWeb. Token counting uses the [Sailor 2 8B](https://huggingface.co/sail/Sailor2-8B-Chat) tokenizer, which includes both English and Vietnamese.

In order to filter toxic content, we use a custom list of flagged words from [Sailcraft](https://github.com/sail-sg/sailcraft) and discard documents with a proportion of flagged words that exceed a threshold.

Since the final training dataset is going to be small, around 2B tokens, we could filter data using even more precise quality signals.
<details>
  <summary>Read more</summary>

We can reuse the data curation method used for [FineWebEdu](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu), by training a fastText language classifier on either:
1. Wikipedia as the high-quality source and Binhvq news corpus as the lower-quality source.
2. Annotations generated by a Vietnamese LLM such as [Sailor 2 8B](https://huggingface.co/sail/Sailor2-8B-Chat). Inspired by [Tournesol](https://tournesol.app/) ([Hoang et al., 2021](https://arxiv.org/abs/2107.07334)), we use multiple criteria and scale the annotations according to adjectives such as recommendable, reliable, pedagogical, important, engaging. For consistency and due to limited resources, we use only one judge for the annotations.

Unfortunately, such an approach is bound to be biased.

Visualization is a good way to ensure the data cleaning process preserves content diversity and overall document length distribution. [This article](https://developer.nvidia.com/blog/processing-high-quality-vietnamese-language-data-with-nvidia-nemo-curator/) provides some code samples used by [Viettel Solutions](https://huggingface.co/VTSNLP/Llama3-ViettelSolutions-8B).

Useful tools for visualization include:
- [text-clustering](https://github.com/huggingface/text-clustering)
</details>

### PII removal

We match public IPs and emails with regular expressions and redact them with a list of placeholder IPs or emails.

We may use:
- [scrubadub](https://scrubadub.readthedocs.io/en/stable/), uses spaCy under the hood. Should work provided the spaCy Vietnamese model is good enough. Tests required.
- [Presidio](https://github.com/microsoft/presidio)

### Data composition

Small scale experiments following the Sailor paper methodology may help find the right dataset sampling and the learning rate.

## Compromises

### Limiting filtering to reduce bias

Both URL-based and quality filtering described above are likely to introduce bias ([Dodge et al., 2021](https://arxiv.org/abs/2104.08758)).

Possible mitigations:
- Use website whitelists and blacklists (keep quality content and filter adult / malicious websites)
- Use more lenient rules in data filtering

### Limiting anonymization to avoid false positives

Regex rules such as for phone numbers are likely to trigger false positives.

### Copyright

The underlying sources of Binhvq news corpus are not in the public domain.

## Further improvements

### Better unbiased data curation

In order to improve generalization capabilities, possibly use toxic identification instead of too much toxicity filtering ([Longpre et al., 2023](https://arxiv.org/abs/2305.13169)).

### Overcome temporal misalignment

The age difference between pretraining and finetuning datasets results in degraded performance ([Longpre et al., 2023](https://arxiv.org/abs/2305.13169)) due to misaligned language use and knowledge.

For our cases, the Binhvq news corpus, CulturaX and MADLAD-400 date back from 2023 or older, FineWeb2-HQ was compiled in 2025. A good metric is to report the temporal distribution of the training dataset.

### Benchmark data decontamination

Removing test samples from the training data would make it possible to compute fair benchmark scores.

### Document-level code-switching

Using a Vietnamese-to-English dictionary for translation, code-switching as described in [Sailor](https://arxiv.org/abs/2404.03608) could help the model align its word representations between English and Vietnamese.

## Citations

TODO: grow the list to all of the pretraining datasets

- [The Lucie-7B LLM and the Lucie Training Dataset: Open resources for multilingual language generation.](https://arxiv.org/abs/2503.12294)
- [CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages](https://aclanthology.org/2024.lrec-main.377)
- [FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language](https://arxiv.org/abs/2506.20920)
- [MADLAD-400 (Multilingual Audited Dataset: Low-resource And Document-level)](https://arxiv.org/abs/2309.04662)
- [Towards Comprehensive Vietnamese Retrieval-Augmented Generation and Large Language Models](https://arxiv.org/abs/2403.01616)
- [MTet: Multi-domain Translation for English and Vietnamese](https://arxiv.org/abs/2210.05610)
- [vbpl.vn - The National Database of Legal Documents, Ministry of Justice, Vietnam](https://vbpl.vn)
- [Enhancing Multilingual LLM Pretraining with Model-Based Data Selection](https://arxiv.org/abs/2502.10361)
- [SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model](https://arxiv.org/abs/2502.02737v1)
- [StackMathQA: A Curated Collection of 2 Million Mathematical Questions and Answers Sourced from Stack Exchange](https://stackmathqa.github.io/StackMathQA.pdf)
- [Sailor: Open Language Models for South-East Asia](https://arxiv.org/abs/2404.03608)
- [Penedo et al. (2024). DataTrove: large scale data processing.](https://github.com/huggingface/datatrove)
- [Robyn Speer. (2019). ftfy (Version 5.5). Zenodo.](http://doi.org/10.5281/zenodo.2591652)
- [Deduplicating Training Data Makes Language Models Better](https://arxiv.org/abs/2107.06499)
- [vietnamese-stopwords](https://github.com/stopwords/vietnamese-stopwords)
